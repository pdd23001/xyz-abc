# Benchwarmer.AI â€” Detailed Explanation

## What Is Benchwarmer.AI?

Benchwarmer.AI is an **AI-powered algorithm benchmarking platform** for graph optimization problems. Instead of manually writing benchmark harnesses, you describe your problem in plain English and the system:

1. **Understands** your problem using an LLM (Claude Sonnet 4)
2. **Generates** a complete benchmark configuration automatically
3. **Runs** your algorithms against diverse graph instances
4. **Visualizes** results interactively through natural language

---

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    run_benchmark.py                      â”‚
â”‚               (Full end-to-end CLI)                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Intake  â”‚  Benchmark   â”‚  Execution   â”‚    Plot         â”‚
â”‚ Agent   â”‚  Config      â”‚  Engine      â”‚    Agent        â”‚
â”‚ (NLâ†’    â”‚  (Pydantic   â”‚  (Runner +   â”‚    (NLâ†’Codeâ†’   â”‚
â”‚  JSON)  â”‚   models)    â”‚   Metrics)   â”‚     Plots)      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚            Foundation Layer                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Generators â”‚ Problem       â”‚ Algorithm             â”‚  â”‚
â”‚  â”‚ (5 types)  â”‚ Classes (2)   â”‚ Interface (ABC)       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Project Structure

```
Benchwarmer.AI/
â”œâ”€â”€ benchwarmer/                  # Main package
â”‚   â”œâ”€â”€ config.py                 # Pydantic data models
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ intake.py             # NL â†’ BenchmarkConfig (Claude)
â”‚   â”‚   â”œâ”€â”€ plot.py               # NL â†’ matplotlib code (Claude)
â”‚   â”‚   â””â”€â”€ tools.py              # Deterministic tool functions
â”‚   â”œâ”€â”€ algorithms/
â”‚   â”‚   â””â”€â”€ base.py               # AlgorithmWrapper ABC
â”‚   â”œâ”€â”€ engine/
â”‚   â”‚   â””â”€â”€ runner.py             # BenchmarkRunner execution engine
â”‚   â”œâ”€â”€ generators/
â”‚   â”‚   â”œâ”€â”€ base.py               # BaseGenerator ABC
â”‚   â”‚   â”œâ”€â”€ erdos_renyi.py        # Random graphs G(n, p)
â”‚   â”‚   â”œâ”€â”€ barabasi_albert.py    # Scale-free networks
â”‚   â”‚   â”œâ”€â”€ grid_2d.py            # 2D grid/lattice graphs
â”‚   â”‚   â”œâ”€â”€ planar_random.py      # Random planar graphs
â”‚   â”‚   â””â”€â”€ planted_partition.py  # Community-structured graphs
â”‚   â”œâ”€â”€ problem_classes/
â”‚   â”‚   â”œâ”€â”€ registry.py           # Auto-discovery registry
â”‚   â”‚   â”œâ”€â”€ maximum_cut.py        # Max-Cut problem definition
â”‚   â”‚   â””â”€â”€ minimum_vertex_cover.py  # Min Vertex Cover definition
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ sandbox.py            # Sandboxed code execution
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_benchmark.py          # Full CLI entry point
â”‚   â””â”€â”€ demo_phase1.py            # Phase 1 standalone demo
â”œâ”€â”€ tests/                        # 47 unit tests
â”‚   â”œâ”€â”€ test_agents.py            # Tool function tests
â”‚   â”œâ”€â”€ test_engine.py            # Runner tests
â”‚   â”œâ”€â”€ test_generators.py        # Generator tests
â”‚   â”œâ”€â”€ test_problem_classes.py   # Problem class tests
â”‚   â””â”€â”€ test_sandbox.py           # Sandbox tests
â”œâ”€â”€ pyproject.toml
â””â”€â”€ requirements.txt
```

---

## How It Works â€” Step by Step

### Step 1: Intake Agent (NL â†’ Configuration)

**File:** `benchwarmer/agents/intake.py`

When you type something like `"max cut problem"`, the Intake Agent:

1. **Classifies** the problem using `classify_problem()` â€” a deterministic keyword-matching tool that scores your description against registered problem classes
2. **Looks up generators** using `get_generators()` â€” retrieves all available graph generators for the matched problem class
3. **Builds a config** â€” Claude assembles a `BenchmarkConfig` JSON with appropriate generators, sizes, and parameters
4. **Validates** using `validate_config()` â€” checks the JSON against the Pydantic schema

The agent uses **Claude Sonnet 4's tool-use** capability. The three tool functions are defined in `agents/tools.py` and are purely deterministic (no API calls), making them independently testable.

**Key design decision:** The system prompt includes an **exact JSON schema example** so Claude generates valid configs on the first try without unnecessary clarifying questions.

### Step 2: Configuration Models

**File:** `benchwarmer/config.py`

The entire benchmark is described by a `BenchmarkConfig` Pydantic model:

```python
BenchmarkConfig
â”œâ”€â”€ problem_class: str              # e.g. "maximum_cut"
â”œâ”€â”€ problem_description: str        # Human-readable
â”œâ”€â”€ objective: "minimize"|"maximize"
â”œâ”€â”€ instance_config: InstanceConfig
â”‚   â””â”€â”€ generators: list[GeneratorConfig]
â”‚       â”œâ”€â”€ type: str               # e.g. "erdos_renyi"
â”‚       â”œâ”€â”€ params: dict            # e.g. {"p": 0.3}
â”‚       â”œâ”€â”€ sizes: list[int]        # e.g. [50, 100, 200, 500]
â”‚       â”œâ”€â”€ count_per_size: int     # default 3
â”‚       â””â”€â”€ why: str                # Reasoning for this generator
â”œâ”€â”€ evaluation_priorities: EvaluationPriorities
â”œâ”€â”€ execution_config: ExecutionConfig
â”‚   â”œâ”€â”€ timeout_seconds: float      # default 60
â”‚   â”œâ”€â”€ runs_per_config: int        # default 5
â”‚   â””â”€â”€ memory_limit_mb: int        # default 2048
â””â”€â”€ solution_validation: SolutionValidation
```

**Notable fix:** `GeneratorConfig.params` uses `validation_alias=AliasChoices("params", "parameters")` because Claude sometimes generates `"parameters"` instead of `"params"`. Without this, the params dict would silently be empty.

### Step 3: Instance Generation

**Files:** `benchwarmer/generators/*.py`

Five graph generators, all subclassing `BaseGenerator`:

| Generator | Description | Key Params | Use Case |
|---|---|---|---|
| **erdos_renyi** | Random graph G(n, p) | `p` (edge probability) | General benchmarks |
| **barabasi_albert** | Preferential attachment | `m` (edges per new node) | Social networks, scale-free |
| **grid_2d** | 2D lattice/grid | â€” | Road networks, structured |
| **planar_random** | Delaunay triangulation | `weighted` | Geographic networks |
| **planted_partition** | Community structure | `k`, `p_in`, `p_out` | Known-optimal testing |

Every generator outputs a **standardized graph dict**:
```python
{
    "nodes": [0, 1, 2, ...],
    "edges": [{"source": 0, "target": 1, "weight": 1.0}, ...],
    "metadata": {"generator": "erdos_renyi", "size": 100, "params": {"p": 0.3}}
}
```

### Step 4: Algorithm Interface

**File:** `benchwarmer/algorithms/base.py`

Users implement the `AlgorithmWrapper` abstract class:

```python
class MyAlgorithm(AlgorithmWrapper):
    name = "my_algo"

    def solve(self, instance: dict, timeout: float = 60.0) -> dict:
        # instance has "nodes", "edges", "metadata"
        # ... your algorithm ...
        return {
            "solution": {"partition": [0, 1, 0, 1, ...]},
            "metadata": {"iterations": 42}
        }
```

The CLI provides **built-in baselines** (greedy + random) for both Max-Cut and Min Vertex Cover so the benchmark runs immediately without custom code.

### Step 5: Execution Engine

**File:** `benchwarmer/engine/runner.py`

`BenchmarkRunner` orchestrates the full benchmark:

1. **Generate instances** â€” iterates over generators Ã— sizes Ã— count_per_size
2. **Run algorithms** â€” for each (algorithm Ã— instance Ã— run_index):
   - Start `tracemalloc` for memory tracking
   - Time the `solve()` call with `time.perf_counter()`
   - Validate the solution via the problem class's `evaluate()` method
   - Record a `BenchmarkResult` with objective value, time, memory, status
3. **Collect results** â€” returns a pandas DataFrame with columns:

| Column | Type | Description |
|---|---|---|
| `algorithm_name` | str | Algorithm identifier |
| `instance_name` | str | e.g. "erdos_renyi_n100_2" |
| `instance_generator` | str | Generator type used |
| `problem_size` | int | Number of nodes |
| `objective_value` | float | Solution quality score |
| `wall_time_seconds` | float | Execution time |
| `peak_memory_mb` | float | Peak memory usage |
| `status` | str | "success", "timeout", or "error" |
| `run_index` | int | Run number (for statistical reliability) |
| `feasible` | bool | Whether the solution is valid |

### Step 6: Problem Classes

**Files:** `benchwarmer/problem_classes/maximum_cut.py`, `minimum_vertex_cover.py`

Each problem class defines:
- **`evaluate(instance, solution)`** â€” computes the objective value
- **`is_feasible(instance, solution)`** â€” checks solution validity
- **`available_generators()`** â€” lists compatible generators
- **`keywords`** â€” for NL classification matching

The **registry** (`registry.py`) auto-discovers problem class modules at import time using `pkgutil`.

### Step 7: Interactive Plot Agent

**File:** `benchwarmer/agents/plot.py`

After benchmarks finish, you enter an interactive analysis loop. Type requests like:
- `"Bar chart comparing average objective by algorithm"`
- `"Box plot of wall time by algorithm for each graph size"`
- `"Summary table of results"`

The Plot Agent:
1. Sends the DataFrame schema + sample rows + your request to Claude
2. Claude generates matplotlib Python code
3. Code runs in the **sandbox** (`utils/sandbox.py`)
4. If it fails, the error is fed back to Claude for **self-correction** (up to 2 retries)
5. The generated code is displayed for transparency

### Step 8: Sandboxed Execution

**File:** `benchwarmer/utils/sandbox.py`

LLM-generated code runs in a **restricted namespace**:
- âœ… Pre-injected: `df`, `plt`, `pd`, `np`, `matplotlib`, `output_path`
- âœ… Safe builtins: `print`, `len`, `range`, `sorted`, `min`, `max`, etc.
- âŒ Blocked: `open()`, `exec()`, `eval()`, `__import__()`, file system access

If no explicit `plt.savefig()` is called, the sandbox auto-saves any open figures.

---

## The Full Pipeline

When you run `python scripts/run_benchmark.py`:

```
1. "Describe your problem" â†’ user types "max cut problem"

2. Intake Agent (4 Claude API calls):
   â†’ classify_problem("max cut problem")     â†’ "maximum_cut" (0.9 confidence)
   â†’ get_generators("maximum_cut")           â†’ 5 generators available
   â†’ validate_config({...})                  â†’ valid âœ…
   â†’ Present config to user

3. Auto-register built-in baselines:
   â†’ greedy_cut (adjacency-based greedy partitioning)
   â†’ random_cut (random 50/50 partition)

4. BenchmarkRunner executes:
   â†’ 4 generators Ã— 4 sizes Ã— 3 instances Ã— 5 runs Ã— 2 algorithms = 480 runs
   â†’ Each run: timed, memory-tracked, solution validated
   â†’ Results: pandas DataFrame

5. Summary table printed

6. Interactive analysis loop:
   ðŸ“Š > "box plot of wall time by size"
   â†’ Claude generates matplotlib code
   â†’ Sandbox executes â†’ plot saved to plots/plot_000.png
   ðŸ“Š > "thanks"
   â†’ Exit
```

---

## Testing

47 unit tests across 5 test files, all deterministic (no API key required):

```
tests/test_agents.py          â€” 14 tests (tool functions, dispatcher)
tests/test_engine.py           â€”  6 tests (runner, metrics, failures)
tests/test_generators.py       â€” 13 tests (all 5 generators)
tests/test_problem_classes.py  â€”  8 tests (Max-Cut, Min Vertex Cover)
tests/test_sandbox.py          â€”  6 tests (execution, errors, safety)
```

Run with: `python -m pytest tests/ -v`

---

## Dependencies

| Package | Purpose |
|---|---|
| `anthropic` | Claude Sonnet 4 API for Intake and Plot agents |
| `pydantic` | Data validation and config models |
| `pandas` | Results DataFrame and analysis |
| `networkx` | Graph generation (all generators use this) |
| `matplotlib` | Plot generation |
| `numpy` | Numerical operations in plots |
| `python-dotenv` | Load API key from `.env` file |

---

## Key Design Decisions

1. **Deterministic tools, stochastic agent** â€” The three Intake Agent tools are pure functions that can be tested without an API key. Only the LLM orchestration layer requires the Anthropic API.

2. **Schema-driven LLM guidance** â€” Instead of hoping the LLM guesses the right JSON format, the system prompt and tool schemas include explicit examples, reducing invalid configs from ~70% to ~0%.

3. **Self-correcting Plot Agent** â€” If generated code fails, the error trace is fed back to Claude, which fixes and retries (up to 2Ã—). This handles edge cases in matplotlib without user intervention.

4. **Sandboxed execution** â€” Generated plot code runs in a restricted namespace with `__builtins__` overridden. This blocks dangerous operations like `open()`, `exec()`, and `__import__()`.

5. **Auto-discovery registries** â€” Both generators and problem classes are auto-discovered via `pkgutil`. Adding a new problem class is as simple as dropping a new `.py` file in `problem_classes/`.

6. **Natural language exit** â€” The analysis loop recognizes phrases like "thanks", "done", "that's enough" â€” not just explicit `exit` commands.
